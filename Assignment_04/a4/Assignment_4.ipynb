{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSK9WzK_qNIP"
      },
      "source": [
        "## Import all relevant libraries essential for completing the tasks in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCbBfySspX1q"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb5TjlY4rgOS"
      },
      "source": [
        "## Q1) Load the Dataset (5)\n",
        "\n",
        "We begin by loading the CNN/DailyMail news summarization dataset, which will be used throughout this assignment evaluate the pre-trained models.\n",
        "\n",
        "This dataset contains news articles paired with human-written summaries, providing a rich source of real-world examples for model development and testing.\n",
        "\n",
        "You can find details about the dataset and instructions for loading it here: https://huggingface.co/datasets/abisee/cnn_dailymail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An2D95iXqV39"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the CNN/DailyMail dataset from huggingface :\n",
        "\n",
        "dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\") # We will use the version 3.0.0 in this assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqhVanezrieD"
      },
      "source": [
        "## Q2) Create a summarization Pipeline (10)\n",
        "\n",
        "In this step, we create a text summarization pipeline using a pre-trained model from the Hugging Face Transformers library.\n",
        "You will be working with two models `\"google-t5/t5-small\"` and `\"sshleifer/distilbart-cnn-12-6\"` .\n",
        "\n",
        "*Note:* Ensure the pipeline is configured to generate summaries with a minimum length of 30 tokens and a maximum of 128 tokens.\n",
        "\n",
        "Find more details about the models and pipelines below:\n",
        "\n",
        "t5-small: https://huggingface.co/google-t5/t5-small\n",
        "\n",
        "distilbart-cnn-12-6: https://huggingface.co/sshleifer/distilbart-cnn-12-6\n",
        "\n",
        "Pipeline: https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.SummarizationPipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNjUVXYGqVmd"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "model_name = \"sshleifer/distilbart-cnn-12-6\" #[\"\"google-t5/t5-small\",\"sshleifer/distilbart-cnn-12-6\"]\n",
        "# TODO:CREATE a pipeline for summarization, in the pipeline set minimum length to 30 and maximum to 128.\n",
        "summarizer = pipeline(\"summarization\", model=model_name, tokenizer=model_name, min_length=30, max_length=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPQr78U4rnWD"
      },
      "source": [
        "## Q3) Summary Generation (10)\n",
        "\n",
        "In this section, you will generate summaries for the first 20 articles from the test split of the CNN/DailyMail dataset using the summarization pipeline you created earlier.\n",
        "\n",
        "For each article, you'll fetch the text, generate a summary with truncation enabled, and then store both the original article and its summary in separate lists.\n",
        "\n",
        "Refer to the example provided here to work with Hugging Face datasets: https://huggingface.co/docs/datasets/en/access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbZtfPXmq6zl"
      },
      "outputs": [],
      "source": [
        "summaries = []\n",
        "articles = []\n",
        "\n",
        "# We will generate summaries for the first 20 articles in the datasetfrom the 'test' split.\n",
        "for i in range(20):\n",
        "    # Get the article from the dataset's test split\n",
        "    article =  dataset[\"test\"][i][\"article\"]\n",
        "\n",
        "    # Generate a summary for the fetched article using the summarization pipeline, set the 'truncation' to 'True' while generatiing sumamries\n",
        "    output =  summarizer(article, truncation=True)\n",
        "\n",
        "    # TODO: Append the generated summary from the output to the summaries list\n",
        "    summaries.append(output[0][\"summary_text\"])\n",
        "\n",
        "    # TODO: Append the original article to the articles list\n",
        "    articles.append(article)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYmkfE7csVYN"
      },
      "source": [
        "## Q4) Evaluating the Summaries (5)\n",
        "\n",
        "In this section, you will evaluate the quality of the generated summaries by comparing them with the reference summaries using the ROUGE metric.\n",
        "\n",
        "Specifically, you will calculate the ROUGE-1 F1 score for each summary and compute the average across all 20 examples to assess overall summarization performance.\n",
        "\n",
        "You can read more about the metric and it's usage here: https://huggingface.co/spaces/evaluate-metric/rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUlyXvtWrZep"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "## TODO: Load the ROUGE metric\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "## We will load the refrence summaries\n",
        "reference_summaries = [dataset[\"test\"][i]['highlights'] for i in range(20)]\n",
        "\n",
        "total_rouge1_f1 = 0\n",
        "# Printing out the F1 scores for ROUGE-1\n",
        "for i, (pred, ref) in enumerate(zip(summaries, reference_summaries)):\n",
        "    # TODO: Compute the ROUGE-1 scores for each summary\n",
        "    result = rouge.compute(predictions=[pred], references=[ref], use_stemmer=True)\n",
        "    rouge_1_f1 = result[\"rouge1\"]\n",
        "\n",
        "    total_rouge1_f1 += rouge_1_f1 # ROGUE_1_F1_SCORE for the summary\n",
        "\n",
        "\n",
        "rouge1_f1_score = total_rouge1_f1 / len(summaries)\n",
        "print(f\"Average ROUGE-1 F1 Score is : {rouge1_f1_score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hQkfVh3IIEn"
      },
      "source": [
        "## Output Storage (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdFnQHYVxMtd"
      },
      "source": [
        "You can store your summaries as shown below, and then repeat the process for the other model.\n",
        "\n",
        "Feel free to use loops or print statements to analyze the five summaries for the written part of the assignment. You can also store them to a csv or JSON and analyze them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0Q1akmmw3XN"
      },
      "outputs": [],
      "source": [
        "# #Ensure the model used for the run before storing them\n",
        "# t5_summaries = summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L9-W8b_xfhz"
      },
      "outputs": [],
      "source": [
        "#Ensure the model used for the run before storing them\n",
        "bart_summaries = summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJGNkGW7yAJz"
      },
      "outputs": [],
      "source": [
        "# Example for analysis\n",
        "for i in range(5):\n",
        "  print(\"Article\", articles[i])\n",
        "  print(\"----------------------------------------------------------------------------------------- \\n\")\n",
        "  # print(\"Summary generated by t5: \", t5_summaries[i])\n",
        "  # print(\"----------------------------------------------------------------------------------------- \\n\")\n",
        "  print(\"Summary generated by Distill-bart: \", bart_summaries[i])\n",
        "  print(\"------XX------XX------XX------XX------XX------XX------XX------XX------XX------XX-------XX \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6Nc2m9oIODm"
      },
      "outputs": [],
      "source": [
        "# Example for storing the summaries in csv file for later analysis\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Article': articles,\n",
        "    # 'T-5 Summary':  t5_summaries,\n",
        "    'Distill-bart Summary': bart_summaries\n",
        "})\n",
        "\n",
        "# df.to_csv('cs421_assgn4_summ_results_t5.csv', index=False)\n",
        "df.to_csv('cs421_assgn4_summ_results_distillbart.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
