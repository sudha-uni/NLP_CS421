{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaOSrvct7dX8"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GwJkoNj7dYA"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets evaluate sacrebleu torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhd_GW5J7dYB"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omSuPoLY7dYB"
      },
      "source": [
        "## Q1: Dataset Preparation (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxECGbqT-pRe"
      },
      "outputs": [],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtQHtK5j7dYB"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRQ7Hsrj7dYC"
      },
      "source": [
        "We use the ```load_dataset()``` function to download the dataset. Replace the dummy arguments to download the wmt14 dataset for fr-en translation as provided here: https://huggingface.co/datasets/wmt/wmt14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klc-jtLi7dYC"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"wmt14\", \"fr-en\", split='train[:15000]')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OajGgO-R7dYC"
      },
      "source": [
        "Now, we split the dataset into training and testing splits. This is done using the ```train_test_split``` function. Replace the dummy arguments with appropriate parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oCpCOAv7dYC"
      },
      "outputs": [],
      "source": [
        "split_datasets = dataset.train_test_split(train_size=0.8, seed=42)\n",
        "split_datasets\n",
        "print(\"Train Size: 0.8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU3rB0eR7dYD"
      },
      "source": [
        "Define the test dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0KvRQL07dYD"
      },
      "outputs": [],
      "source": [
        "test_dataset = split_datasets[\"test\"]\n",
        "test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykFRxwh57dYD"
      },
      "source": [
        "Now, follow the same process to split the train dataset to training and validation splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kynutdwu7dYD"
      },
      "outputs": [],
      "source": [
        "split_to_val = split_datasets[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
        "train_dataset = split_to_val[\"train\"]\n",
        "eval_dataset = split_to_val[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J7AypmH7dYD"
      },
      "source": [
        "## Q2 Prepare for training RNNs (10)\n",
        "In this part, you are required to define the tokenizers for english and french, tokenize the data, and define the dataloaders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaR1qKZs7dYD"
      },
      "source": [
        "Choose and initialize the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfFHDk3D7dYD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\") # CHOOSE AN APPROPRIATE MULTILINGUAL MODEL such as https://huggingface.co/google-bert/bert-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITKX-cza7dYD"
      },
      "source": [
        "You will need to create a pytorch dataset to process the tokens in the required format. Complete the implementation of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCKYlKu07dYD"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataset, input_size, output_size):\n",
        "        source_texts = [text[\"translation\"][\"fr\"] for text in dataset]\n",
        "        target_texts = [text[\"translation\"][\"en\"] for text in dataset]\n",
        "        self.source_sentences = tokenizer(source_texts, padding='max_length', truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        self.target_sentences = tokenizer(target_texts, padding='max_length', truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.source_sentences[idx], self.target_sentences[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-JfiZAH7dYE"
      },
      "source": [
        "Get the vocab size from the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR1CEzJN7dYE"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.vocab_size # This size is used somewhere in the model, think."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUl1vQj07dYE"
      },
      "source": [
        "Initialize the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-Hqb62L7dYE"
      },
      "outputs": [],
      "source": [
        "train_dataset_rnn = TranslationDataset(split_to_val[\"train\"], vocab_size, vocab_size)\n",
        "eval_dataset_rnn = TranslationDataset(split_to_val[\"test\"], vocab_size, vocab_size)\n",
        "test_dataset_rnn = TranslationDataset(test_dataset, vocab_size, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uvrc0R7dYE"
      },
      "source": [
        "Initialize and define the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W3qVX2N7dYE"
      },
      "outputs": [],
      "source": [
        "#Instantiate the DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 8\n",
        "train_dataloader = DataLoader(train_dataset_rnn, batch_size=BATCH_SIZE, shuffle=True)\n",
        "eval_dataloader = DataLoader(eval_dataset_rnn, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_dataset_rnn, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X03nj-qd7dYE"
      },
      "source": [
        "## Q3: Implementing RNNs (10)\n",
        "Define the RNN model as an encoder-decoder RNN for the task of translation in the cell below. You may refer: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iYwjZXt7dYE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZcyOabn7dYE"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.decoder = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, hidden = self.encoder(embedded)\n",
        "        decoder_output, _ = self.decoder(embedded, hidden)\n",
        "        output = self.fc(decoder_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnP9NUtK7dYE"
      },
      "outputs": [],
      "source": [
        "model = Seq2SeqRNN(input_size = tokenizer.vocab_size, hidden_size= 256, output_size= tokenizer.vocab_size)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MZb5OZc7dYE"
      },
      "source": [
        "## Q4: Training RNNs (15)\n",
        "In this question, you will define the hyperparameters, loss and optimizer for training. You will then implement a custom training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INYhxibp7dYE"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7cPIUCd7dYE"
      },
      "source": [
        "define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeCoP4DC7dYE"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "num_train_epochs = 5\n",
        "num_training_steps = num_train_epochs * len(train_dataloader)\n",
        "criterion = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2w-eirJ7dYE"
      },
      "source": [
        "Write the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gsDdRV_UrRa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "waBwBdVx7dYE"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_src, batch_tgt in train_dataloader:\n",
        "        ## Complete the training loop\n",
        "        batch_src, batch_tgt = batch_src.cuda(), batch_tgt.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_src)\n",
        "\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), batch_tgt.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch}: Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch_src, batch_tgt in eval_dataloader:\n",
        "      batch_src, batch_tgt = batch_src.cuda(), batch_tgt.cuda()\n",
        "\n",
        "      output = model(batch_src)\n",
        "      loss = criterion(output.view(-1, output.shape[-1]), batch_tgt.view(-1))\n",
        "\n",
        "      total_eval_loss += loss.item()\n",
        "      total_batches += 1\n",
        "\n",
        "      ### Complete the evaluation phase\n",
        "\n",
        "    avg_loss = total_eval_loss / total_batches\n",
        "    print(f\"Epoch {epoch}: Average Eval Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7rzcB5NfjQdV"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"seq2seq_rnn.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gUJdFziYjY_Q"
      },
      "outputs": [],
      "source": [
        "model = Seq2SeqRNN(input_size = tokenizer.vocab_size, hidden_size = 256, output_size = tokenizer.vocab_size)\n",
        "model.load_state_dict(torch.load(\"seq2seq_rnn.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-e1x_Lm7dYF"
      },
      "source": [
        "## Q5: Evaluating RNNs for Machine Translation (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrE3V_P17dYF"
      },
      "source": [
        "Implement the calculation of BLEU-1,2,3,4 scores using the ```sacrebleu``` library for the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xddoMFY17dYF"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "progress_bar = tqdm(total=len(test_dataloader), desc=\"Testing Progress\")\n",
        "\n",
        "preds = []\n",
        "for batch_src, batch_tgt in tqdm(test_dataloader):\n",
        "    output = model(batch_src) #similar to the eval loop\n",
        "    out_seq =  output.argmax(dim=-1).squeeze().tolist() #Note that the outputs you get from the model will be probabilities for all possible tokens in the vocabulary. You need to take the argmax to get the most likely token for each position in the output sequence before passing it to the tokenizer decode. squeeze() and tolist() functions convert it to the right format.\n",
        "    outs = tokenizer.batch_decode(out_seq, skip_special_tokens=True)  #batch_decoding to convert the list of token ids to a list of strings\n",
        "    preds.extend(outs)\n",
        "    progress_bar.update(1)\n",
        "\n",
        "targets = [] #you need to fill this in. Similar to how it is done earlier while defining the dataset. Also remember that the targets need to be a list of lists as evident from the documentation of the sacrebleu metric.\n",
        "for _, batch_tgt in test_dataloader:\n",
        "    tgt_texts = tokenizer.batch_decode(batch_tgt.tolist(), skip_special_tokens=True)\n",
        "    targets.extend([[text] for text in tgt_texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1NjzPSbVe3_"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJEbcllgVJJa"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "\n",
        "sacrebleu = load(\"sacrebleu\") #load the sacrebleu metric\n",
        "scores = sacrebleu.compute(predictions=preds, references=targets)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpaHAQWp7dYF"
      },
      "source": [
        "Congratulations! You can now work with RNNs for the task of Machine Translation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeBy8PO37dYF"
      },
      "source": [
        "## Q6: Prepare for training transformers (10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vauBDs_a7dYJ"
      },
      "source": [
        "In this part we cover the initial setup required before training transformer this including data preprocessing and setting up data collators and loaders.\n",
        "\n",
        "Ensure you have loaded the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW5Y83Ew7dYK"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMGCBjca7dYK"
      },
      "source": [
        "We will begin by tokenizing the data. Based on your model selection load the appropriate tokenizer. We are using models from AutoModelForSeq2SeqLM in this assignment. You can checkout all the available models here: https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr49H_Bq7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"google-t5/t5-small\" #Select a model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_FwXurd7dYK"
      },
      "source": [
        "We will need to tokenize both our input and outputs. Thus we make use of pre_process() function to generate tokenized model inputs and targets. Ensure you use truncation and padding! The max length will be 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF8_JKSS7dYK"
      },
      "outputs": [],
      "source": [
        "##Implement the preprocess function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [example[\"fr\"] for example in examples[\"translation\"]]\n",
        "    targets = [example[\"en\"] for example in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\") #Instantitate tokenizer to generate model outputs\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IwNWQEf8Fru"
      },
      "outputs": [],
      "source": [
        "tokenized_train_data = train_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H42Bthkh8GHS"
      },
      "outputs": [],
      "source": [
        "tokenized_val_data = eval_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9KiU8Uw8jpI"
      },
      "source": [
        "We remove the column 'translation' as we do not require it for training. Also often having columns other than we created using the preprocess_function may lead to errors during training. Since model might get confused which inputs it needs to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z_WfysF8Jqz"
      },
      "outputs": [],
      "source": [
        "tokenized_train_data = tokenized_train_data.remove_columns(train_dataset.column_names)\n",
        "tokenized_val_data = tokenized_val_data.remove_columns(eval_dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES0vpD308KFK"
      },
      "outputs": [],
      "source": [
        "tokenized_train_data.set_format(\"torch\")\n",
        "tokenized_val_data.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBKQon-E7dYK"
      },
      "source": [
        "To construct batches of training data for model training, we require collators that set the properties for the batches and data loaders that generate the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J_Bs4bT7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint) #INSTANTIATE THE COLLATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ErHyp6A7dYK"
      },
      "outputs": [],
      "source": [
        "#Instantiate the DataLoader for training and evaluation data\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_train_data, batch_size=32, shuffle=True)\n",
        "eval_dataloader = DataLoader(tokenized_val_data, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq75QwZr7dYK"
      },
      "source": [
        "## Q7) Choosing & Loading the Model (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Cr5Z0T7dYK"
      },
      "source": [
        "Choose a pre-trained transformer model that you will use for fine-tuning on the translation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZY0-m7L7dYK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsaL0EAC7dYK"
      },
      "source": [
        "## Q8) Training the Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKlkWxxh7dYK"
      },
      "source": [
        "Now, that we have are data tokenized and ready in batches and model fixed. We will begin with training this model. To do so we must setup the right hyperparameters, then proceed to implment the training loop to train our model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e0JWoMi7dYL"
      },
      "source": [
        "For training we require an optimizer and a scheduler to manage the learning rate during the training. Let's set them up before our training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axjaE8qWldv2"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxwICUp17dYL"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_training_steps = len(train_dataloader) * num_train_epochs\n",
        "\n",
        "optimizer = AdamW(model.parameters())\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqdgWb_Q7dYL"
      },
      "source": [
        "Finally, we are here!\n",
        "\n",
        "In the loop during training you will run a forward pass, compute the loss, compute the gradients, and then update the weights. (Don't foregt to set gradient to zero!)\n",
        "\n",
        "During the eval phase we simply do a forward pass and compute the loss!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0iDbp0x7dYL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        ## Complete the training loop\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Evaluation Phase\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        ### Complete the evaluation phase\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    print(f\"Epoch {epoch}: Average Eval Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_7JA8gD7dYL"
      },
      "source": [
        "Congratulations!! On completing the training. Now don't forget to save your model and the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqTmBLMG7dYL"
      },
      "outputs": [],
      "source": [
        "# Save model and tokenizer\n",
        "output_dir = \"./trained_model\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCrwRPf-7dYL"
      },
      "source": [
        "## Q9) Evaluating Transformer for Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsteZEw77dYL"
      },
      "source": [
        "We will now test our trained model and analyze its performance using BLEU-1, 2, 3, 4 scores from the sacrebleu library. You will create a task evaluator for translation, load and process the test dataset, and compute the results on an existing trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNP8tJ3Y9Yna"
      },
      "source": [
        "Below we load a model trained for french to english translation. You can read more about it here: https://huggingface.co/Helsinki-NLP/opus-mt-tc-big-fr-en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUIYns7E9S2j"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "checkpoint = \"Helsinki-NLP/opus-mt-tc-big-fr-en\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HRtKCKh9mqc"
      },
      "source": [
        "Initialize an evaluator for translation task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR_Kc8NN7dYL"
      },
      "outputs": [],
      "source": [
        "## Load Evaluator for translation\n",
        "from evaluate import evaluator\n",
        "task_evaluator = evaluator(\"translation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL8zPaFz7dYL"
      },
      "source": [
        "We will need to change our test dataset by having specific input and target columns. Thus we will use split_translation to split the translation column into two columns 'en' and 'fr'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMCG74Vr7dYL"
      },
      "outputs": [],
      "source": [
        "#  Implement the split function\n",
        "def split_translations(example):\n",
        "    en_text = example[\"translation\"][\"en\"]\n",
        "    fr_text = example[\"translation\"][\"fr\"]\n",
        "    example['en'] = en_text\n",
        "    example['fr'] = fr_text\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpUlKUm17dYL"
      },
      "outputs": [],
      "source": [
        "test_data = test_dataset.map(split_translations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBntiYyVqOP7"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"fr\"]\n",
        "    targets = examples[\"en\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_test_data = test_data.map(preprocess_function, batched=True)\n",
        "tokenized_test_data = tokenized_test_data.remove_columns(test_data.column_names)\n",
        "tokenized_test_data.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA9Vt7kF7dYL"
      },
      "source": [
        "You can now go ahead and compute the results by appropriately setting up the task_evaluator.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV6gQhFP7dYL"
      },
      "outputs": [],
      "source": [
        "results = task_evaluator.compute(\n",
        "    model_or_pipeline=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data=test_data,\n",
        "    metric=\"sacrebleu\",\n",
        "    input_column=\"fr\",\n",
        "    label_column=\"en\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEv4BBNq7dYM"
      },
      "outputs": [],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvPJif197dYM"
      },
      "source": [
        "## Q10) Inferencing on Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2VpQtGW7dYM"
      },
      "source": [
        "Let's check out how well this trained model's translation skills are. You can use try with a few french sentence and see how well it translates.\n",
        "\n",
        "To do so we will setup a pipline using the existing trained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6mVfvWC7dYM"
      },
      "source": [
        "Loading the tokenizer and model for the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nojJlgVf7dYM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "checkpoint = \"Helsinki-NLP/opus-mt-tc-big-fr-en\"\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53K25pUn7dYM"
      },
      "source": [
        "Setup the pipeline for translation using your model and tokenizer. You can read about pipelines here: https://huggingface.co/docs/transformers/en/main_classes/pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKp9pNoQ7dYM"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "# Instatiate a pipeline for Translation using the model and tokenizer\n",
        "pipeline = pipeline(\n",
        "    \"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdV-e6sM7dYM"
      },
      "source": [
        "Translate the given sentence using the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc8Y01YC7dYM"
      },
      "outputs": [],
      "source": [
        "input_text = \"Vous avez maintenant terminÂ´e le deuxi`eme devoir de ce cours.\"\n",
        "translation_result = pipeline(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eRCXrNN7dYM"
      },
      "outputs": [],
      "source": [
        "print(translation_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VavKO-Qnu7Bh"
      },
      "outputs": [],
      "source": [
        "input_text = \"Jâ€™ai traduit cette phrase du francais vers lâ€™anglais.\"\n",
        "translation_result = pipeline(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3cCBi0Xu9WW"
      },
      "outputs": [],
      "source": [
        "print(translation_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkLkTTZfu-4W"
      },
      "outputs": [],
      "source": [
        "input_text = \"Chicago est cÂ´el`ebre pour ses pizzas profondes, son jazz et son architecture Â´epoustouflante.\"\n",
        "translation_result = pipeline(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQniRrmOu_FJ"
      },
      "outputs": [],
      "source": [
        "print(translation_result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}